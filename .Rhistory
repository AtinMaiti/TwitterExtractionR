++i;
i
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "utf8towcs && ASCII")) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, text.default) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation()) # remove punct.
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
install.packages("wordcloud")
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "utf8towcs && ASCII")) # use the corpus function
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
for(i in 1:length(udemylist))
{
Encoding(corpus[[i]]) = "UTF-8"
}
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
for(i in 1:length(udemylist))
{
Encoding(udemycorpus[[i]]) = "UTF-8"
}
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
for(i in 1:length(udemylist))
for(i in 1:length(udemylist))
{
Encoding(udemycorpus[[i]]) = "UTF-8"
}
udemylist <- sapply(udemytweets, function(x) iconv(enc2utf8(x), sub = "byte") # initiating a function
# in depth info about the apply family and functions in the course "R Level 1"
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
# a corpus is the text body consisting of all the text including the meta info
for(i in 1:length(udemylist))
{
Encoding(udemycorpus[[i]]) = "UTF-8"
}
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
# there is a link to a stop word list in the link lecture
# Lets see which other transformations tm offers
?getTransformations
# to trasform to plain text which wordcloud can use
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
# changing to a tdm
udemytdm <- TermDocumentMatrix(udemycorpus)
# a DocumentTermMatrix is a very useful tool when it comes to text mining
# it structures the text in a matrix where each term is organized in a column
# each row is a document and the number represents the counts of that term
udemytdm
# frequent terms
findFreqTerms(udemytdm, lowfreq=11)
?findFreqTerms
# associations
findAssocs(udemytdm, 'android', 0.60)
# Lets get a dendrogram to see related terms
# Remove sparse (infrequently used) terms from the term-document matrix
udemy2tdm <-removeSparseTerms(udemytdm, sparse=0.9)
# Lets scale the data
udemy2tdmscale <- scale(udemy2tdm)
# distance matrix
udemydist <- dist(udemy2tdmscale, method = "euclidean")
# hierarchical clustering
udemyfit <- hclust(udemydist)
# Visualize the result
plot(udemyfit)
# to calculate a certain number of groups
cutree(udemyfit, k=6)
# we can even color the 6 groups and plot them
rect.hclust(udemyfit, k=6, border="red")
udemylist <- sapply(udemytweets, function(x) iconv(enc2utf8(x), sub = "byte")) # initiating a function
udemylist <- sapply(udemytweets, function(x) iconv(x, to='UTF-8-MAC', sub='byte')) # initiating a function
udemylist <- sapply(udemytweets, function(x) x$iconv(x, to='UTF-8-MAC', sub='byte')) # initiating a function
udemycorpus <- tm_map(yourCorpus, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
udemycorpus <- tm_map(udemycorpus, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
udemycorpus <- tm_map(udemylist, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
udemycorpus <- tm_map(udemytweets, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "ASCII")) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- Corpus(VectorSource(udemylist), readerControl = list(language = "english")) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
udemytdm <- TermDocumentMatrix(udemycorpus)
udemytdm
findFreqTerms(udemytdm, lowfreq=11)
?findFreqTerms
findAssocs(udemytdm, 'android', 0.60)
udemy2tdm <-removeSparseTerms(udemytdm, sparse=0.9)
udemy2tdmscale <- scale(udemy2tdm)
udemydist <- dist(udemy2tdmscale, method = "euclidean")
udemyfit <- hclust(udemydist)
plot(udemyfit)
cutree(udemyfit, k=6)
rect.hclust(udemyfit, k=6, border="red")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
1
udemytweets = searchTwitter("#Udemy", n=1000)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
for( i in udemylist)
if udemylist[i] == "utf8towcs"
for( i in udemytweets)
if (udemytweets[i] == "utf8towcs")
{
udemytweets[i] <- NULL
}
for( i in udemytweets)
if (udemytweets[i] == "utf8towcs")
{
udemytweets[i] <- NULL
}
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
1
udemytweets = searchTwitter("#Udemy", n=1000)
for( i in udemytweets)
if (udemytweets[i] == "utf8towcs")
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
library("twitteR")
library("httr")
1
udemytweets = searchTwitter("#Udemy", n=1000)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
udemytweets = searchTwitter("#Udemy", n=1000)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
udemytweets = searchTwitter("#Udemy", n=1000)
class(udemytweets)
iconv(udemytweets$text, "ASCII", "UTF-8", sub="")
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
udemytdm <- TermDocumentMatrix(udemycorpus)
udemytdm
findFreqTerms(udemytdm, lowfreq=11)
?findFreqTerms
findAssocs(udemytdm, 'android', 0.60)
udemy2tdm <-removeSparseTerms(udemytdm, sparse=0.9)
udemy2tdmscale <- scale(udemy2tdm)
udemydist <- dist(udemy2tdmscale, method = "euclidean")
udemyfit <- hclust(udemydist)
plot(udemyfit)
cutree(udemyfit, k=6)
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
udemytweets = searchTwitter("#Udemy", n=1000)
iconv(udemytweets$text, "ASCII", "UTF-8", sub="")
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
udemycorpus
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
? wordcloud
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
?wordcloud
wordcloud(udemycorpus, min.freq=2, scale=c(5,1), random.color=F, max.word=45, random.order=F)
wordcloud(udemycorpus, min.freq=2, scale=c(3,1), random.color=F, max.word=45, random.order=F)
udemytdm <- TermDocumentMatrix(udemycorpus)
udemytdm
findFreqTerms(udemytdm, lowfreq=11)
?simple_triplet_matrix
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
##https://apps.twitter.com/app
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
key <- "n4RBNeOKXgjF97pp93tv0vAK6"
secret <- "cazafkzwfHlacuqvBy0FAdEKGyZqN3F1jPIwpeTIDDdrMreoJV"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
udemytweets = searchTwitter("#udemy", n=1000)
iconv(udemytweets$text, "ASCII", "UTF-8", sub="")
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
udemycorpus
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
?wordcloud
?simple_triplet_matrix
wordcloud(udemycorpus, min.freq=2, scale=c(3,1), random.color=F, max.word=45, random.order=F)
a = c('a', 'b', 'c')
b = c(1, 2, 3)
c = c('e','f')
d = 6
glist = list(a,b,c,d)
glist[2][3]
glist[[2]][3]
glist[[2]][3] = d
glist
glist
glist[[2]][3] = f
mtcars
str(mtcars)
mtcars[[3]]
mtcars[[3]][14]
key <- "dPf0GNX6WZg9AN50W8mYTxw60"
secret <- "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- 	"861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
udemytweets = searchTwitter("#udemy", n=1000)
setup_twitter_oauth(key,secret,mytoken,secrettk)
udemytweets = searchTwitter("#udemy", n=1000)
udemytweets
str(udemytweets)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
key <- "dPf0GNX6WZg9AN50W8mYTxw60"
secret <- "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk <- "JPUh0ovIuZLMLhOwmY4hgsiiNmZp62cNr3XJ5pYH9FuqV"
mytoken <- "861271732480618496-QnM6qtZ1MIYCKRlA8oqIIJwFaFJWUQU"
library("twitteR")
library("httr")
setup_twitter_oauth(key,secret,mytoken,secrettk)
udemytweets = searchTwitter("#udemy", n=1000)
iconv(udemytweets$text, "ASCII", "UTF-8", sub="")
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
udemycorpus
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
?wordcloud
?simple_triplet_matrix
wordcloud(udemycorpus, min.freq=2, scale=c(3,1), random.color=F, max.word=45, random.order=F)
udemytdm <- TermDocumentMatrix(udemycorpus)
udemytdm
findFreqTerms(udemytdm, lowfreq=11)
?findFreqTerms
findAssocs(udemytdm, 'android', 0.60)
udemy2tdm <-removeSparseTerms(udemytdm, sparse=0.9)
udemy2tdmscale <- scale(udemy2tdm)
udemydist <- dist(udemy2tdmscale, method = "euclidean")
udemyfit <- hclust(udemydist)
plot(udemyfit)
cutree(udemyfit, k=6)
rect.hclust(udemyfit, k=6, border="red")
key <- "vYKJtmEaonRNkH42Hc9TyeIYN"
secret <- "nYqAuqWpXkvsbEDvU8GQ029OlcRjzpx5xI5bY5qEiwpINahTTl"
secrettk <- "UQChGugAym3FSFQjfyioCgXY5aBQhgPOkJQIjfyhgiXFp"
mytoken <- "861271732480618496-jU05ARZuD3jSpjMfgUMMtKXMfMDpQB"
library("twitteR")
library("httr")
setup_twitter_oauth(key,secret,mytoken,secrettk)
key <- "vYKJtmEaonRNkH42Hc9TyeIYN"
secret <- "nYqAuqWpXkvsbEDvU8GQ029OlcRjzpx5xI5bY5qEiwpINahTTl"
secrettk <- "UQChGugAym3FSFQjfyioCgXY5aBQhgPOkJQIjfyhgiXFp"
mytoken <- "861271732480618496-jU05ARZuD3jSpjMfgUMMtKXMfMDpQB"
library("twitteR")
library("httr")
setup_twitter_oauth(key,secret,mytoken,secrettk)
udemytweets = searchTwitter("#udemy", n=1000)
library("twitteR")
library("ROAuth")
key = "dPf0GNX6WZg9AN50W8mYTxw60"
secret = "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
setwd("C:/Users/maiti/Desktop/TwitterExtractionR")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="C:/Users/maiti/Desktop/TwitterExtractionR/cacert.pem",
method="auto")
?OAuthFactory
authenticate <-  OAuthFactory$new(consumerKey=key,
consumerSecret=secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
authenticate$handshake(cainfo="C:/Users/maiti/Desktop/TwitterExtractionR/cacert.pem")
save(authenticate, file="twitter authentication.Rdata")
registerTwitterOAuth(authenticate)
setup_twitter_oauth(key, secret)
library("twitteR")
library("httr")
library("ROAuth")
setup_twitter_oauth(key, secret, mytoken, secrettk)
setup_twitter_oauth(key, secret, secrettk, mytoken)
udemytweets = searchTwitter("#Udemy", n=1000)
library("twitteR")
library("httr")
library("twitteR")
library("httr")
library("ROAuth")
key = "dPf0GNX6WZg9AN50W8mYTxw60"
secret = "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk = "F9PDbqUgvrUECyJPNmjDKu6RvUBFyIXEf7un1x55TQti6"
mytoken = "861271732480618496-k1lR51w18VsCnSVi7o1Tha3oV3mMHUh"
setup_twitter_oauth(key, secret, mytoken, secrettk)
udemytweets = searchTwitter("#Udemy", n=1000)
library("twitteR")
library("httr")
setup_twitter_oauth(key,secret,mytoken,secrettk)
udemytweets = searchTwitter("#udemy", n=1000)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
library("twitteR")
library("httr")
library("ROAuth")
key = "dPf0GNX6WZg9AN50W8mYTxw60"
secret = "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk = "F9PDbqUgvrUECyJPNmjDKu6RvUBFyIXEf7un1x55TQti6"
mytoken = "861271732480618496-k1lR51w18VsCnSVi7o1Tha3oV3mMHUh"
setup_twitter_oauth(key, secret, mytoken, secrettk)
udemytweets = searchTwitter("#", n=1000)
udemytweets = searchTwitter("#Dope", n=1000)
class(udemytweets)
length(udemytweets)
library("tm")
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
lapply(tweettext, function(x) iconv(x, "latin1", "ASCII", sub=""))
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
lapply(udemylist, function(x) iconv(x, "latin1", "ASCII", sub=""))
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemylist <- sapply(udemytweets, function(x) x$getText()) # initiating a function
udemylist <- lapply(udemylist, function(x) iconv(x, "latin1", "ASCII", sub=""))
udemycorpus <- Corpus(VectorSource(udemylist)) # use the corpus function
udemycorpus <- tm_map(udemycorpus, tolower) # putting text to lower case
udemycorpus <- tm_map(udemycorpus, removePunctuation) # remove punct.
udemycorpus <- tm_map(udemycorpus, function(x) removeWords(x,stopwords())) # remove stopwords (meaningless words)
udemycorpus
?getTransformations
udemycorpus <- tm_map(udemycorpus, PlainTextDocument)
library("wordcloud")
?wordcloud
?simple_triplet_matrix
wordcloud(udemycorpus, min.freq=2, scale=c(3,1), random.color=F, max.word=45, random.order=F)
wordcloudwordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
wordcloud(udemycorpus, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F)
library("twitteR")
library("httr")
library("ROAuth")
key = "dPf0GNX6WZg9AN50W8mYTxw60"
secret = "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk = "F9PDbqUgvrUECyJPNmjDKu6RvUBFyIXEf7un1x55TQti6"
mytoken = "861271732480618496-k1lR51w18VsCnSVi7o1Tha3oV3mMHUh"
setup_twitter_oauth(key, secret, mytoken, secrettk)
tweets = searchTwitter("#Dope", n=1000)
class(udemytweets)
class(tweets)
length(tweets)
library("tm")
tweettext = sapply(tweets, function(x) x$getText()) # extracting the text, tweets is the object your scraped from
tweettext=lapply(tweettext, function(x) iconv(x, "latin1", "ASCII", sub="")) # conversion to standard characters # tweettext=lapply(tweettext, function(x) gsub("htt.*",' ',x)) # this step is optional, it removes urls and anything after it
tweettext=lapply(tweettext, function(x) gsub("#",'',x)) # removing the hashes
tweettext=unlist(tweettext) # getting the character vector
tweettext=tolower(tweettext) # putting words to lower cases
wordcloud(tweettext, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F) # plotting the wordcloud with a simple vector
udemytdm <- TermDocumentMatrix(udemycorpus)
tdm <- TermDocumentMatrix(tweettext)
library("twitteR")
library("httr")
library("ROAuth")
key = "dPf0GNX6WZg9AN50W8mYTxw60"
secret = "xn46cHIOFaQDp5TTDhR7jPj5YT64ks4VC9vMrBRiuZxjPRu4Xv"
secrettk = "F9PDbqUgvrUECyJPNmjDKu6RvUBFyIXEf7un1x55TQti6"
mytoken = "861271732480618496-k1lR51w18VsCnSVi7o1Tha3oV3mMHUh"
setup_twitter_oauth(key, secret, mytoken, secrettk)
tweets = searchTwitter("#DataScience", n=1000)
class(tweets)
length(tweets)
library("tm")
tweettext = sapply(tweets, function(x) x$getText()) # extracting the text, tweets is the object your scraped from
tweettext=lapply(tweettext, function(x) iconv(x, "latin1", "ASCII", sub="")) # conversion to standard characters # tweettext=lapply(tweettext, function(x) gsub("htt.*",' ',x)) # this step is optional, it removes urls and anything after it
tweettext=lapply(tweettext, function(x) gsub("#",'',x)) # removing the hashes
tweettext=unlist(tweettext) # getting the character vector
tweettext=tolower(tweettext) # putting words to lower cases
wordcloud(tweettext, min.freq=4, scale=c(5,1),
random.color=F, max.word=45, random.order=F) # plotting the wordcloud with a simple vector
